{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER: MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>\n",
        "\n",
        "To copy the notebook, go to File and click create \"Save a copy to ...\" and work on that copy.\n",
        "\n",
        "Please submit a pdf of the page of your notebook (Ctrl + p on the page, save as pdf, and submit that pdf) on gradescope.\n",
        "\n",
        "Please remember to assign pages to the appropriate questions. Not doing so will result in the deduction of points. Please submit a **pdf** version of the colab notebook.\n",
        "\n",
        "We will not rerun your uploaded notebook, so make sure to run each cell before downloading so that all outputs and plots are visible on the saved pdf.\n"
      ],
      "metadata": {
        "id": "YAUtuvQ-YPWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework 5: In this homework, you will train a two layer neural network using gradient descent. However, instead of manually computing the gradients, you will use the autodiff provided by Tensorflow package."
      ],
      "metadata": {
        "id": "d89fHnC4YR5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 0 Part 1\n",
        "Do you have confusions or questions about the previous lectures?  (This is optional to answer)"
      ],
      "metadata": {
        "id": "o0zxifKYRySU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Answer here)"
      ],
      "metadata": {
        "id": "165w3P08R1gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 0 Part 2\n",
        "Any suggestions or thoughts about the course? (This is optional to answer)"
      ],
      "metadata": {
        "id": "5KhcWc2rR4dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Answer here)"
      ],
      "metadata": {
        "id": "I1SQl4hVR5Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "5MLZbUp8aYfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two-layer Neural Network"
      ],
      "metadata": {
        "id": "tQ6iFPm0Zn8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A two layer neural network contains an input layer, a hidden layer, and an output layer. The number of the input nodes in the diagram below is determined the dimension of our features. We are free to choose the dimension of the hidden layer. As for the final output layer, the number of nodes is determined by the type of the problem we are doing. For instance, for a regression problem, we will only have one node and the output value corresponds to our prediction of the target. As for classification, we will first output a vector that has same number of dimension as the number of classes in our classification dataset. Then, we will apply the softmax transformation on the vector to transform real-valued predictions to the class probabilities.\n",
        "![](https://www.researchgate.net/profile/Haohan-Wang-4/publication/282997080/figure/fig4/AS:305939199610886@1449952997594/A-typical-two-layer-neural-network-Input-layer-does-not-count-as-the-number-of-layers-of.png)\n",
        "\n",
        "Mathematically, this model can be written as\n",
        "\n",
        "$$f(x) =  \\sigma(x^{\\intercal} W_1  +b_1) W_2 + b_2. $$\n",
        "\n",
        "Note that if $x \\in \\mathbb{R}^{d \\times 1}$, then $W_1 \\in \\mathbb{R}^{d \\times H}$ and $W_2 \\in \\mathbb{R}^{H \\times O}$, where $O$ is the output dimension. The dimension of $b_1$ and $b_2$ is self-evident.\n",
        "\n",
        "Given an input $x$, the model first transforms it using the weight matrix $W_1$ and subsequently shifts the output by the bias term $b_1$. The function $\\sigma(.)$ is called the activation function that introdues non-linearity in the model. For the purpose of this homework, we will use the so called relu-activation function that is defined as $\\sigma(t) = \\text{max}\\{t, 0\\}$. Note that that $x^{\\intercal} W_1  +b_1$ generally gives us a vector, so we have to apply the relu activation to each element of the vector. Following the activation, the vector $h(x) =\\sigma(x^{\\intercal} W_1  +b_1)$ is defines the hidden layer. Finally, we apply the linear trasformation $ h(x) W_1 + b_2$ on the hidden layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "SPPcL-_mCZgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This representation of the network is very convenient if you instead want to do matrix operations on your data. Suppose $X$ be your data matrix where $i^{th}$ row of $X$ contains $x_i^{\\intercal}$, then the output of the network on the entire dataset can be written as\n",
        "$$f(X) = \\sigma(X W_1  +b_1) W_2 + b_2. $$"
      ],
      "metadata": {
        "id": "dFuaUzk4WPco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous HW, you trained a linear regression model on california housing dataset. In this homework, you will train a two-layer neural network on this dataset using gradient descent."
      ],
      "metadata": {
        "id": "VRXIXnYXGM_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "california_housing = fetch_california_housing( return_X_y=True, as_frame=True)\n",
        "X = california_housing[0]\n",
        "y = california_housing[1]\n",
        "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train_unscaled)\n",
        "X_test = sc.transform(X_test_unscaled)"
      ],
      "metadata": {
        "id": "uqhXKxB8equj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert numpy arrays to tensors\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train, dtype = tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype = tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype = tf.float32)\n",
        "y_test= tf.convert_to_tensor(y_test, dtype = tf.float32)"
      ],
      "metadata": {
        "id": "CEjywXczhLNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 1: Fill in the input dimension and output dimension of the two layer neural network for regression on this dataset. (5 pts) </b></h6></font>\n",
        "For the purpose of this homework, we will just choose hidden layer with dimension double that of input dimension. However, going forward choosing the hidden dimension appropriately would be an important part of deep learning."
      ],
      "metadata": {
        "id": "T2xrmNPqHUvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace ______ with your code\n",
        "input_dim = X_train.shape[1]\n",
        "# print(input_dim)\n",
        "hidden_dim = 2 * input_dim\n",
        "# print(hidden_dim)\n",
        "output_dim = 1\n",
        "# print(output_dim)"
      ],
      "metadata": {
        "id": "byb5Y_HlIKkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 2: Define a tensorflow variables for weights W1, b1, W2, and b2. Then, initialize both biases b1 and b2 to be 0 vectors and initialize W1 and W2 by picking values uniformly at random from the interval [0,1]. (10 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "KF_mIPodILR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace ______ with your code\n",
        "W1 = tf.Variable(tf.random.uniform(shape=(input_dim, hidden_dim)), trainable=True)\n",
        "b1 = tf.Variable(tf.zeros(hidden_dim))\n",
        "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim, output_dim)), trainable=True)\n",
        "b2 = tf.Variable(tf.zeros(output_dim))"
      ],
      "metadata": {
        "id": "3TcJV1q0gRpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b1.shape"
      ],
      "metadata": {
        "id": "OdYUj6S4Hddy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78746d81-8927-4281-acea-0bd423b8925f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 3: Complete the model function below to define a two layer neural network. Here, inputs is a matrix of shape $n \\times d$, where $i^{th}$ row of the inputs matrix contains $x_i^{\\intercal}$. (15 pts) </b></h6></font>\n",
        "Hint: Use [tf.nn.relu()](https://www.tensorflow.org/api_docs/python/tf/nn/relu) function for relu activation."
      ],
      "metadata": {
        "id": "VuGkzh54JC3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "def model(inputs):\n",
        "  # print(\"model\")\n",
        "  hidden = tf.nn.relu(tf.add(tf.matmul(inputs, W1), b1))    # hidden layer with relu-activation\n",
        "  # print(\"model2\")\n",
        "  # print(\"Hidden shape : \", hidden.shape)\n",
        "  # print(\"W2 shape : \", W2.shape)\n",
        "  output = hidden @ W2 + b2            # fully-connected output linear layer\n",
        "  # print(\"Output shape : \", output.shape)\n",
        "  # print(\"model end\")\n",
        "  return output"
      ],
      "metadata": {
        "id": "uD_Rc4AxgS_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 4: Write a function below that computes mean-squared error given the predictions and targets. You should use tensorflow operations like tf.square and tf.reduce_mean for autodiff to work.  (15 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "hzj0hUiiK_DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "def mse(predictions, targets):\n",
        "  # print(\"mse\")\n",
        "  # print(predictions.shape)\n",
        "  # print(targets.shape)\n",
        "  squared_error = tf.square(tf.subtract(predictions, targets))\n",
        "  # print(\"Mse2\")\n",
        "  mean_squared_error = tf.reduce_mean(squared_error)\n",
        "  # print(\"mse end\")\n",
        "  return mean_squared_error"
      ],
      "metadata": {
        "id": "1uSkG_ixcLWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 5: Complete the function below that takes in features and targets and trains your two layer neural network using gradient descent. Please use autodiff by Gradient taping.  (25 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "v0NCJNXELmK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: This pseudocode should give you an essence of what is expected from your solution\n",
        "```\n",
        "learning_rate = 0.01\n",
        "def train(inputs, targets):\n",
        "  start your tape: #You need to write a statement here that intializes a gradient tape\n",
        "    make predictions using model function in question 3\n",
        "    observe loss of your predictions using mse function in question 4\n",
        "  grad_w1, grad_b1, grad_w2, grad_b2 = tape the gradient of both weights and biases\n",
        "  w1 = w1 - learning_rate*grad_w1 # use the appropriate tf functions for this\n",
        "  Do the above step for w2, b1, b2 with its respective gradients\n",
        "\n",
        "    \n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w_kiBsNvVkOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"________\" with your code\n",
        "learning_rate = 0.01\n",
        "\n",
        "def train(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # print(\"train\")\n",
        "      predictions = model(inputs)           #get predictions\n",
        "      loss = mse(predictions, targets)           #compute loss\n",
        "    dW1, db1, dW2, db2 = tape.gradient(loss, [W1, b1, W2, b2])        #tape the gradients of both weights and biases\n",
        "\n",
        "    #update all weights and biases using the gradients computed above\n",
        "    W1.assign_sub(dW1 * learning_rate)\n",
        "    b1.assign_sub(db1 * learning_rate)\n",
        "    W2.assign_sub(dW2 * learning_rate)\n",
        "    b2.assign_sub(db2 * learning_rate)\n"
      ],
      "metadata": {
        "id": "9D55l5QAc_Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 6: Complete the routine below that divides the randomly shuffled data into multiple minibatches of size 500 and use the train function above to run gradient descent on those minibatches. Within each step, you should make a complete pass through the dataset. (25 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "ZCI8zqrL_aFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: This pseudocode should give you an essence of what is expected from your solution\n",
        "```\n",
        "k = number of minibatches of size 500\n",
        "for step in range(100):\n",
        "  X = ...\n",
        "  y = ...\n",
        "  for i in range(k): #for the minibatch iterations\n",
        "    run the train method on your ith minibatch\n",
        "  run the train method on the last minibatch with less than 500 samples\n",
        "  compute loss\n",
        "    \n",
        "```"
      ],
      "metadata": {
        "id": "R7lWlx7TZ983"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 1000\n",
        "n = X_train.shape[0]\n",
        "k = int(n/B)\n",
        "for step in range(100):\n",
        "    X_train = tf.random.shuffle(X_train, seed = step)   #separately shuffling X_train and y_train is reasonable here because of the same seed\n",
        "    y_train = tf.random.shuffle(y_train, seed = step)\n",
        "    xbatches = [X_train[j * B:(j + 1)*B] for j in range(k)]\n",
        "    ybatches = [y_train[j * B:(j + 1)*B] for j in range(k)]\n",
        "    # print(xbatches)\n",
        "    #############################################################################\n",
        "    #                             Write your code here                          #\n",
        "    for i in range(k):\n",
        "      # print(i)\n",
        "      # print(xbatches[i].dtype)\n",
        "      # print(ybatches[i].dtype)\n",
        "      train(xbatches[i], ybatches[i])\n",
        "    xbatches = X_train[k * B:(k + 1)*B]\n",
        "    ybatches = y_train[k * B:(k + 1)*B]\n",
        "    # print(xbatches.shape)\n",
        "    # print(ybatches.shape)\n",
        "    train(xbatches, ybatches)\n",
        "    #############################################################################\n",
        "    loss = mse(model(X_train), y_train)\n",
        "    if (step+1) % 10 == 0:\n",
        "      print(f\"Loss at step {step}: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "gqrMxyB_dkB0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "426d557a-0109-4dd6-aee8-09a008e5e673"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step 9: 1.3406\n",
            "Loss at step 19: 1.3405\n",
            "Loss at step 29: 1.3405\n",
            "Loss at step 39: 1.3404\n",
            "Loss at step 49: 1.3403\n",
            "Loss at step 59: 1.3403\n",
            "Loss at step 69: 1.3404\n",
            "Loss at step 79: 1.3403\n",
            "Loss at step 89: 1.3402\n",
            "Loss at step 99: 1.3403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 7: Get predictions on your test data set and report the MSE loss on the test data.  (5 pts) </b></h6></font>"
      ],
      "metadata": {
        "id": "zHEgstAJMqrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "y_pred = model(X_test)\n",
        "mse(y_test, y_pred).numpy()"
      ],
      "metadata": {
        "id": "MNNF9Sf9lwAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dccc5921-de6d-4d9d-fed3-577399893a40"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3131732"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    }
  ]
}