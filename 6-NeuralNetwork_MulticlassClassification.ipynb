{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUtuvQ-YPWc"
   },
   "source": [
    "<font color=\"#de3023\"><h1><b>REMINDER: MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>\n",
    "\n",
    "To copy the notebook, go to File and click create \"Save a copy to ...\" and work on that copy.\n",
    "\n",
    "Please submit a pdf of the page of your notebook (Ctrl + p on the page, save as pdf, and submit that pdf) on gradescope.\n",
    "\n",
    "Please remember to assign pages to the appropriate questions. Not doing so will result in the deduction of points. Please submit a **pdf** version of the colab notebook.\n",
    "\n",
    "We will not rerun your uploaded notebook, so make sure to run each cell before downloading so that all outputs and plots are visible on the saved pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d89fHnC4YR5a"
   },
   "source": [
    "Homework 6: In this homework, you will train a two layer neural network for multiclass classification using gradient descent. However, instead of manually computing the gradients, you will use the autodiff provided by Tensorflow package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNcS0I2MZztH"
   },
   "source": [
    "## Question 0 Part 1\n",
    "Do you have confusions or questions about the previous lectures?  (This is optional to answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTzUKUYfZ2MH"
   },
   "source": [
    "(Answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r60QBSPTZ5J1"
   },
   "source": [
    "## Question 0 Part 2\n",
    "Any suggestions or thoughts about the course? (This is optional to answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RZ-xU6VZ8XR"
   },
   "source": [
    "(Answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5MLZbUp8aYfE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ6iFPm0Zn8U"
   },
   "source": [
    "# Two-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPPcL-_mCZgZ"
   },
   "source": [
    "\n",
    "A two layer neural network contains an input layer, a hidden layer, and an output layer. The number of the input nodes in the diagram below is determined the dimension of our features. We are free to choose the dimension of the hidden layer. As for the final output layer, the number of nodes is determined by the type of the problem we are doing. For instance, for a regression problem, we will only have one node and the output value corresponds to our prediction of the target. As for classification, we will first output a vector that has same number of dimension as the number of classes in our classification dataset. Then, we will apply the softmax transformation on the vector to transform real-valued predictions to the class probabilities.\n",
    "![](https://www.researchgate.net/profile/Haohan-Wang-4/publication/282997080/figure/fig4/AS:305939199610886@1449952997594/A-typical-two-layer-neural-network-Input-layer-does-not-count-as-the-number-of-layers-of.png)\n",
    "\n",
    "Mathematically, this model can be written as\n",
    "\n",
    "$$f(x) =  \\sigma(x^{\\intercal} W_1  +b_1) W_2 + b_2. $$\n",
    "\n",
    "Note that if $x \\in \\mathbb{R}^{d \\times 1}$, then $W_1 \\in \\mathbb{R}^{d \\times H}$ and $W_2 \\in \\mathbb{R}^{H \\times O}$, where $O$ is the output dimension. The dimension of $b_1$ and $b_2$ is self-evident.\n",
    "\n",
    "Given an input $x$, the model first transforms it using the weight matrix $W_1$ and subsequently shifts the output by the bias term $b_1$. The function $\\sigma(.)$ is called the activation function that introdues non-linearity in the model. For the purpose of this homework, we will use the so called relu-activation function that is defined as $\\sigma(t) = \\text{max}\\{t, 0\\}$. Note that that $x^{\\intercal} W_1  +b_1$ generally gives us a vector, so we have to apply the relu activation to each element of the vector. Following the activation, the vector $h(x) =\\sigma(x^{\\intercal} W_1  +b_1)$ is defines the hidden layer. Finally, we apply the linear trasformation $ h(x) W_1 + b_2$ on the hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFuaUzk4WPco"
   },
   "source": [
    "This representation of the network is very convenient if you instead want to do matrix operations on your data. Suppose $X$ be your data matrix where $i^{th}$ row of $X$ contains $x_i^{\\intercal}$, then the output of the network on the entire dataset can be written as\n",
    "$$f(X) = \\sigma(X W_1  +b_1) W_2 + b_2. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eO5l34GHZrDQ"
   },
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfaxLYw4XDp-"
   },
   "source": [
    "You will train the two layer neural network to do multiclass classification on digits dataset. Digits data is similar to MNIST  but has even smaller pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PJJii4cQxn_9"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y_int = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "neyuUY77dLcW",
    "outputId": "b8a18276-fe84-4d4a-ef2b-571e4655302c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "gs = gridspec.GridSpec(8, 8)\n",
    "gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "for i, img in enumerate(X[:64,:]):\n",
    "  ax = plt.subplot(gs[i])\n",
    "  plt.axis(\"off\")\n",
    "  ax.set_xticklabels([])\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_aspect(\"equal\")\n",
    "  plt.imshow(img.reshape([8, 8]), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqD6t8NRXXRd"
   },
   "source": [
    "In previous homeworks, you manually created an one hot encoding of the multiclass targets. We can use a function from keras, which is a high level deep learning API thats works pretty well with tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yjf5hOJny-CH"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_one_hot = to_categorical(y_int, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0zQwnabK0_vW"
   },
   "outputs": [],
   "source": [
    "# training-testing split and appropriate rescaling\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train_unscaled)\n",
    "X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I19f0o141D4-"
   },
   "outputs": [],
   "source": [
    "#convert to tensors\n",
    "X_train = tf.convert_to_tensor(X_train, dtype = tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype = tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype = tf.float32)\n",
    "y_test= tf.convert_to_tensor(y_test, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98zNQ6TkYMbV"
   },
   "source": [
    "<font color=\"#de3023\"><h6><b>Question 1: Fill in the input dimension and output dimension of the two layer neural network appropriate for this dataset. (5 pts) </b></h6></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjjRU7A1kGyr"
   },
   "source": [
    "Remember the input dimension is the number of features in the vector $x$ and the output dimension is the number final outputs in $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xIUHivlC1Gvd"
   },
   "outputs": [],
   "source": [
    "# Replace \"________\" with your code\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = X_train.shape[1]*2\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziX9ZaDUYTd1"
   },
   "source": [
    "<font color=\"#de3023\"><h6><b>Question 2: Define  tensorflow variables for  weights W1, b1, W2, and b2. Then, initialize both biases b1 and b2 to be 0 and initialize W1 and W2 by picking values uniformly at random from the interval [0, 0.1].  (5 pts) </b></h6></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMK86m8pkJo9"
   },
   "source": [
    "Remember that in order the matrix products make sense, the dimension of the operants have to match. For example to calculate $x^{\\intercal} W_1  +b_1 $, $W_1$ has to have the same number of rows as the number of features in the vector $x$ and $b_1$ has to have the same dimension as the resulting vector  $x^{\\intercal} W_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPuxqPqdi7zw"
   },
   "source": [
    "Another importan thing to keep in mind is that while using tensorflow, we need to work with Variables when we want to learn parameters such as $W_1, b_1, W_2$ and $b_2$, otherwise the automatic differentiation API of Tensorflow won't calculate the corresponding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2EZe8nmIYTsN"
   },
   "outputs": [],
   "source": [
    "# Replace \"________\" with your code\n",
    "W1 = tf.Variable(tf.random.uniform(shape=(input_dim, hidden_dim), minval = 0, maxval = 0.1), trainable=True)\n",
    "b1 = tf.Variable(tf.zeros(shape = (hidden_dim)))\n",
    "W2 = tf.Variable(tf.random.uniform(shape=(hidden_dim, output_dim), minval = 0, maxval = 0.1), trainable=True)\n",
    "b2 = tf.Variable(tf.zeros(shape = (output_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WF4RjiNZxO_"
   },
   "source": [
    "<font color=\"#de3023\"><h6><b>Question 3: Complete the classifier function below to define a two layer neural network that outputs class probabilities for each class. Here, inputs is a matrix of shape $n \\times d$, where $i^{th}$ row of the inputs matrix contains $x_i^{\\intercal}$. (10 pts) </b></h6></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SC-eN8lmQmc"
   },
   "source": [
    "$$f(X) = \\sigma(X W_1  +b_1) W_2 + b_2. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8sSySsamTMl"
   },
   "source": [
    "Hint: use [tf.nn.relu function](https://www.tensorflow.org/api_docs/python/tf/nn/relu) as the activation function $\\sigma$, and use [tf.softmax.nn()](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) function to compute softmax class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hjqk697V1MJW"
   },
   "outputs": [],
   "source": [
    "# Replace \"________\" with your code\n",
    "def classifier(inputs):\n",
    "  hidden = tf.nn.relu(tf.add(tf.matmul(inputs, W1), b1))    # hidden layer with relu-activation\n",
    "  # print(\"hidden\", hidden)\n",
    "  linear = tf.add(tf.matmul(hidden, W2), b2) # fully-connected  linear layer\n",
    "  # print(\"linear\", linear)\n",
    "  softmax = tf.nn.softmax(linear)   #softmax layer\n",
    "  # print(\"softmax\", softmax)\n",
    "  # print(\"softmax reduce sum 0\", tf.reduce_sum(softmax, 0)) # (, 10) different total probabiliy for each label\n",
    "  # print(\"softmax reduce sum 1\", tf.reduce_sum(softmax, 1)) # (1257,) all 1, cause total of softmax across label is 1\n",
    "  # print(\"Softmax wrong : \", tf.debugging.check_numerics(softmax, message = \"softmax \"))\n",
    "  # print(\"HAHAHHAHA\", tf.reduce_sum(softmax, 0))\n",
    "  return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Tub82FUbLkD"
   },
   "source": [
    "<font color=\"#de3023\"><h6><b>Question 4: Complete the  function below to compute cross entropy loss given one hot encoding of targets and predictions with class probabilities for each prediction. (10 pts) </b></h6></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo9tTcTHo8wp"
   },
   "source": [
    "\\begin{align*}\n",
    "\\widehat{y}_{ij} &= softmax (x_i)_j \\\\\n",
    "  L(W) &= -\\frac{1}{n}\\sum_{i =1}^n\\sum_{j = 1}^{m}y_{ij}\\log(\\widehat{y}_{ij}) \\\\\n",
    "  &= -\\frac{1}{n} Y * \\log(\\widehat{Y})\n",
    "\\end{align*}\n",
    "\n",
    "Where $*$ represents the elementwise product between 2 arrays and $\\log$ is applied elementwise as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JHX2NPkw1NWr"
   },
   "outputs": [],
   "source": [
    "# Replace \"________\" with your code\n",
    "def cross_entropy(predictions, targets):\n",
    "  # print(predictions)\n",
    "  # print(targets)\n",
    "  # print(targets * tf.math.log(predictions))\n",
    "  # print(tf.rekduce_sum(targets * tf.math.log(predictions), axis = 1))\n",
    "  # print(\"log(predictions) wrong : \", tf.debugging.check_numerics(tf.math.log(predictions), message = \"log predictions\"))\n",
    "  cr_entropies =  tf.reduce_sum(targets * tf.math.log(predictions))\n",
    "  # print(\"cre\", cr_entropies)\n",
    "  # print(\"Cross entropies wrong : \", tf.debugging.check_numerics(cr_entropies, message = \"cross entropies\"))\n",
    "  mean_cr_entropy =  - 1/ predictions.shape[0] * (cr_entropies)\n",
    "  # mean_cr_entropy =  - tf.reduce_mean(cr_entropies)\n",
    "  # print(\"mcre\", mean_cr_entropy)\n",
    "  return mean_cr_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EHBjhOnbpmM"
   },
   "source": [
    "<font color=\"#de3023\"><h6><b>Question 5: Complete the function below to train the neural network classifier using gradient descent. (5 pts) </b></h6></font> Hint: Use the function [tape.gradient](https://www.tensorflow.org/api_docs/python/tf/GradientTape#gradient) to calculate the gradients with the automatic differentiation API of Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uj-VqhnwziR"
   },
   "source": [
    "$$W_1 = W_1 - \\eta \\nabla_{W_1}L$$\n",
    "\n",
    "$$b_1 = b_1 - \\eta \\nabla_{b_1}L$$\n",
    "\n",
    "$$W_2 = W_2 - \\eta \\nabla_{W_2}L$$\n",
    "\n",
    "$$b_2 = b_2 - \\eta \\nabla_{b_2}L$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yJfNhf4E2wVw"
   },
   "outputs": [],
   "source": [
    "# Replace \"________\" with your code\n",
    "learning_rate = 0.1\n",
    "\n",
    "def train(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = classifier(inputs) #get predictions\n",
    "      loss = cross_entropy(predictions, targets) #compute loss\n",
    "    dW1, db1, dW2, db2 = tape.gradient(loss, [W1, b1, W2, b2]) #tape the gradients of both weights and biases\n",
    "    # print(dW1, db1, dW2, db2)\n",
    "    #update all weights and biases using the gradients computed above\n",
    "    W1.assign_sub(dW1 * learning_rate)\n",
    "    b1.assign_sub(db1 * learning_rate)\n",
    "    W2.assign_sub(dW2 * learning_rate)\n",
    "    b2.assign_sub(db2 * learning_rate)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHU_8kZ8cBlW"
   },
   "source": [
    "Train the model using 500 GD iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PMUUJyA20q8",
    "outputId": "581b9da1-a6e7-4dfe-ee37-ef22febd4774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 9: 2.1881\n",
      "Loss at step 19: 2.0532\n",
      "Loss at step 29: 1.8796\n",
      "Loss at step 39: 1.6656\n",
      "Loss at step 49: 1.4302\n",
      "Loss at step 59: 1.2008\n",
      "Loss at step 69: 0.9961\n",
      "Loss at step 79: 0.8214\n",
      "Loss at step 89: 0.6783\n",
      "Loss at step 99: 0.5667\n",
      "Loss at step 109: 0.4821\n",
      "Loss at step 119: 0.4177\n",
      "Loss at step 129: 0.3682\n",
      "Loss at step 139: 0.3292\n",
      "Loss at step 149: 0.2978\n",
      "Loss at step 159: 0.2721\n",
      "Loss at step 169: 0.2506\n",
      "Loss at step 179: 0.2323\n",
      "Loss at step 189: 0.2165\n",
      "Loss at step 199: 0.2026\n",
      "Loss at step 209: 0.1904\n",
      "Loss at step 219: 0.1796\n",
      "Loss at step 229: 0.1699\n",
      "Loss at step 239: 0.1612\n",
      "Loss at step 249: 0.1534\n",
      "Loss at step 259: 0.1463\n",
      "Loss at step 269: 0.1398\n",
      "Loss at step 279: 0.1338\n",
      "Loss at step 289: 0.1284\n",
      "Loss at step 299: 0.1233\n",
      "Loss at step 309: 0.1186\n",
      "Loss at step 319: 0.1142\n",
      "Loss at step 329: 0.1101\n",
      "Loss at step 339: 0.1063\n",
      "Loss at step 349: 0.1027\n",
      "Loss at step 359: 0.0993\n",
      "Loss at step 369: 0.0961\n",
      "Loss at step 379: 0.0931\n",
      "Loss at step 389: 0.0903\n",
      "Loss at step 399: 0.0875\n",
      "Loss at step 409: 0.0850\n",
      "Loss at step 419: 0.0825\n",
      "Loss at step 429: 0.0802\n",
      "Loss at step 439: 0.0780\n",
      "Loss at step 449: 0.0758\n",
      "Loss at step 459: 0.0738\n",
      "Loss at step 469: 0.0718\n",
      "Loss at step 479: 0.0699\n",
      "Loss at step 489: 0.0681\n",
      "Loss at step 499: 0.0664\n"
     ]
    }
   ],
   "source": [
    "for step in range(500):\n",
    "    loss = train(X_train, y_train)\n",
    "    if (step +1)%10==0:\n",
    "      print(f\"Loss at step {step}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hVr6AALcRMB"
   },
   "source": [
    "Suppose, given the vector of class probabilities, you output the label with the highest class probability as your label. As an evaluation of our model, we want to compute the number of mistakes that the model makes. For instance, if $y$ is the true label and $\\widehat{y}$ is the prediction of the model, we will evaluate our model on this point with 0-1 loss\n",
    "$$\\mathbb{1}( \\widehat{y} \\neq y ).$$\n",
    "Over $n$ points, we will compute the mean 0-1 loss,\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}( \\widehat{y}_i \\neq y_i ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_yAWkZqeV3-"
   },
   "source": [
    "<font color=\"#de3023\"><h6><b>Question 6: Compute the mean 0-1 loss of your classifier on the test data. (5pts) </b></h6></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnZltPxu_DpR",
    "outputId": "89f25f8d-4409-4938-dca4-49b181a62985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9685184955596924\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier(X_test)\n",
    "\n",
    "# print(y_pred)\n",
    "# print(y_test)\n",
    "y_label = tf.argmax(y_pred, axis = 1)\n",
    "y_test_label = tf.argmax(y_test, axis = 1)\n",
    "# print(y_label)\n",
    "# print(y_test_label)\n",
    "matches = y_label == y_test_label\n",
    "# print(matches)\n",
    "mean = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "print(f\"accuracy: {mean}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
